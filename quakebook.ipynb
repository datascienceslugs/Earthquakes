{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Earthquake prediction using Random Forests\n*Anders Poirel - 08/05/2019*\n"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tsfresh as tsf\nimport glob \nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To-do:** show code from notebook with EDA."},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n\nFunctions for extraction predictors, response variables and test predictors from a **sample** of the dataset (by default, we sample every 100th entry in the `train.csv`)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictors(filepath, col_name, seg_length, data_len, skip_amount = 100):\n    series_df = pd.read_csv(filepath, \n                            usecols = [col_name],\n                            dtype = np.float32\n                            skiprows = range(1, data_len, skip_amount)\n                           )\n    num_points = len(series_df.index)\n    interval_length = num_points // (seg_length // skip_amount)\n    num_segs = num_points // interval_length \n\n    id_col = np.empty((num_points ,1))\n    for i in range(num_segs):\n        id_col[i] = i // interval_length\n\n    series_df['id'] = id_col\n\n    from tsfresh.feature_extraction import MinimalFCParameters\n\n    predictors = tsf.extract_features(series_df,\n                                      column_value = col_name,\n                                      column_id = 'id',\n                                     )\n    del series_df\n    gc.collect()\n\n    return predictors.values[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_responses(filepath, col_name, seg_len, data_len, skip_amount = 100):\n\n    series_df = pd.read_csv(filepath, \n                            usecols = [col_name],\n                            dtype = np.float32,\n                            skiprows = range(1, data_len, skip_amount)\n                           )\n    num_points = len(series_df.index)\n    interval_len = num_points // (seg_len // skip_amount)\n    num_segs = num_points // interval_len\n\n    response = np.empty(num_segs)\n    for i in range(num_segs):\n        response[i] = series_df.iloc[interval_len * (i+1), 0]\n    del series_df\n    gc.collect()\n\n    return response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_predictors(file_directory, col_name, seg_length, \n                        skip_amount = 100):\n\n    test_predictors = []\n    \n    for fname in glob.glob(file_directory):\n        seg_df = pd.read_csv(fname, skiprows = range(1,seg_length, skip_amount))\n        id_col = np.zeroes(len(seg_df.index))\n        seg_df['id'] = id_col\n        temp_predictors = tsf.extract_features(seg_df,\n                                               column_value = col_name,\n                                               column_id = 'id')\n        test_predictors.append(temp_predictors.iloc[1:])\n\n    del seg_df\n    gc.collect()\n\n    return np.array(test_predictors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Length of the training and test data files. We use these valus to simplify data extraction."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_LEN = 621985673 # counted using  a test run\nSEG_LEN = 150001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract the data from `train.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = get_responses('../input/train.csv',\n                        col_name = 'time_to_failure',\n                        seg_len = SEG_LEN,\n                        data_len = DATA_LEN)\n\n\nX_train = get_predictors('../input/train.csv',\n                         col_name = 'acoustic_data',\n                         seg_len = SEG_LEN,\n                         data_len = DATA_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the results to a `.csv` file to simplify future modeling work."},{"metadata":{"trusted":true},"cell_type":"code","source":"numpy.savetxt(\"y_train.csv\", y_train, delimiter = \",\")\nnumpy.savetxt(\"X_train.csv\", X_train, delimiter = \",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Fit and Testing\nParameters for fitting decision trees. I used the recommendations in ISL as a guide to selecting these:"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_DEPTH = 5\nMAX_FEATURES = 'sqrt'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth = MAX_DEPTH, max_features = MAX_FEATURES)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model diagnostics\nCompute accuracy of model predictions on training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred = rf.predict(X_train)\naccuracy_score(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Building the submission "},{"metadata":{},"cell_type":"markdown","source":"Extract test data and use model to make predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_train = get_test_predictors(col_name = 'acoustic_data')\ny_test_pred = rf.predict(X_test_train)\n\nseg_names = []\nfor fname in glob.glob():\n    seg_names.append(fname)\n\nsubmission = pd.DataFrame('seg_id': seg_names, 'time_to_failure': y_test_pred)\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Possible improvements\n* using the entire dataset instead of a sample using methods for reducing the size of the data\n* using scikit-learn's built-in parameter selection, which would require re-writing the entire data pipeline. Effect of any tuning using k-fold cross-validation will likely be minor as we are using random forests.\n* building a strategy that leverages more the structure of the data - the data shows 17 earthquakes, and looks pseudo-periodical. I don't yet know how we can do this"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}